# Optimize and Deploy Deep Learning Models on Edge Devices

Welcome to our official repository for the graduation project titled **"Optimize and Deploy Deep Learning Models on Edge Devices"**, developed by:
  
**Mentored by:** Si-Vision  

---

## ğŸ“˜ Abstract

This project explores efficient deployment of deep learning models on resource-constrained edge devices, particularly the NVIDIA Jetson Nano. It addresses computational, memory, and energy limitations using a full pipeline of optimization techniques, including pruning, quantization, knowledge distillation, low-rank approximation, and TensorRT acceleration. We evaluated this pipeline on architectures like VGG11, MobileNetV2, EfficientNet, NASNet, and AlexNet, achieving up to 18Ã— speedup with minimal accuracy loss.

---

## ğŸ“‚ Table of Contents

- [Project Overview](#project-overview)
- [Optimized Models](#optimized-models)
- [Optimization Techniques](#optimization-techniques)
- [Deployment Tools](#deployment-tools)
- [Results](#results)
- [Environment](#environment)
- [Contributors](#contributors)

---

## ğŸš€ Project Overview

The projectâ€™s objective is to deploy deep learning models efficiently on the Jetson Nano using a scalable optimization pipeline. The goal is to retain near-original accuracy while achieving substantial reductions in model size and inference time.

---

## ğŸ§  Optimized Models

- **AlexNet**
- **VGG11 & VGG16**
- **MobileNetV2**
- **EfficientNet-B0**
- **NASNet-Mobile**
- **YOLOv8s**
- **LLMs** (e.g., Qwen2.5-0.5B, TinyLlama-1.1B)

---

## âš™ï¸ Optimization Techniques

### ğŸ”§ Pruning
- Structured channel pruning
- Iterative pruning
- FastNAS pruning
- Torch-Pruning with dependency graphs

### ğŸ“ Quantization
- Post-training quantization (PTQ)
- Quantization-aware training (QAT)
- TensorRT precision calibration (INT8, FP16)

### ğŸ§Š Low-Rank Approximation
- SVD-based layer compression
- CP-like decomposition for Conv2D

### ğŸ§ª Knowledge Distillation
- Student-teacher training with NASNet
- Used for both classification and detection tasks

---

## ğŸ§° Deployment Tools

- **TensorRT** for high-speed inference
- **ONNX Runtime** as baseline
- **PyTorch** & **TensorFlow Lite** as training and conversion tools

---

## ğŸ“ˆ Results

- **MobileNetV2:** 9.88MB â†’ 0.47MB, 31.9 FPS â†’ 66.1 FPS, 92.6% accuracy retained  
- **VGG11:** 7Ã— speedup with only 2% accuracy drop  
- **YOLOv8s:** Significant mAP improvements via distillation and quantization  
- **LLMs:** Benchmarked TinyLlama, DeepSeek, and others on Jetson Nano

ğŸ“Š **Full Excel Results Sheet:** [View on OneDrive](https://docs.google.com/spreadsheets/d/1Db0EXINeAfmpou3PeocLsoQTGAfQ5cSlHY8U9TEEHKI/edit?gid=0#gid=0)

(See detailed tables in `/results` and thesis for full metrics)

---

## ğŸ§ª Environment

- **Device:** NVIDIA Jetson Nano
- **Frameworks:** PyTorch, TensorFlow Lite, ONNX, TensorRT
- **Datasets:** CIFAR-10, ImageNet, COCO, Pascal VOC


---
